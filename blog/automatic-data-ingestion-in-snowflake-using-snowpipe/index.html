<!doctype html><html><head><title>Automatic Data Ingestion in Snowflake Using Snowpipe | /vishnuvp</title><meta charset=utf-8><meta name=language content="en"><meta name=description content><meta name=keywords content="snowpipe ,AWS ,S3 ,snowpipe ,automatic-data-loading ,automatic-data-ingestion ,cloud-datawarehouse ,data-lake ,SQS ,SQL ,snowsql ,external-stage ,etl ,extract ,load ,transform ,data-pipeline ,dwaas ,cloud"><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=twitter:card content="summary"><meta name=twitter:title content="Automatic Data Ingestion in Snowflake Using Snowpipe"><meta name=twitter:description content><meta name=twitter:site content="@https://twitter.com/vishnuvp"><meta name=twitter:creator content="@https://twitter.com/vishnuvp"><link rel="shortcut icon" type=image/png href=/favicon.ico><link rel=stylesheet href=/css/post.min.f2df1c9158216eb0f6fdfeacee4a0ee821dee43ee7e09951e8f503899b51e02d.css integrity="sha256-8t8ckVghbrD2/f6s7koO6CHe5D7n4JlR6PUDiZtR4C0="><link rel=stylesheet href=/css/custom.min.60deead5f67cbad66103c82dac86ac2168012087d1c5a12ea299bc027b26d1da.css integrity="sha256-YN7q1fZ8utZhA8gtrIasIWgBIIfRxaEuopm8Ansm0do="><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"\/"},"articleSection":"blog","name":"Automatic Data Ingestion in Snowflake Using Snowpipe","headline":"Automatic Data Ingestion in Snowflake Using Snowpipe","description":"","inLanguage":"en-US","author":"","creator":"","publisher":"","accountablePerson":"","copyrightHolder":"","copyrightYear":"2020","datePublished":"2020-07-03 15:52:12 \u002b0530 \u002b0530","dateModified":"2020-07-03 15:52:12 \u002b0530 \u002b0530","url":"\/blog\/automatic-data-ingestion-in-snowflake-using-snowpipe\/","wordCount":"1480","keywords":["snowpipe","AWS","S3","snowpipe","automatic-data-loading","automatic-data-ingestion","cloud-datawarehouse","data-lake","SQS","SQL","snowsql","external-stage","etl","extract","load","transform","data-pipeline","dwaas","cloud","Blog"]}</script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-41294744-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-41294744-2','auto');ga('send','pageview');}</script></head><body><div class=burger__container><div class=burger aria-controls=navigation aria-label=Menu><div class="burger__meat burger__meat--1"></div><div class="burger__meat burger__meat--2"></div><div class="burger__meat burger__meat--3"></div></div></div><nav class=nav role=navigation><ul class=nav__list><li><a class=active href=/blog/>blog</a></li><li><a href=/>home</a></li></ul></nav><main><div class=flex-wrapper><div class=post__container><div class=post><header class=post__header><h1 id=post__title>Automatic Data Ingestion in Snowflake Using Snowpipe</h1><time datetime="2020-07-03 15:52:12 +0530 +0530" class=post__date>Jul 3 2020</time></header><article class=post__content><p>Snowflake is the new buzzword in the cloud world, which offers data-warehouse-as-a-service (probably DWAAS in the cloud lingo). If you would like to read more about its capabilities and use cases around it, take a look at their site <a href=https://www.snowflake.com/>here</a>. FYI, they have built a new SQL Engine from scratch, and the whole platform runs on AWS infrastructure. It is a technological irony that Snowflake performs better (in my opinion) than Amazon Redshift in many ways.</p><p>That&rsquo;s all about Snowflake intro!</p><p>This post is a simple tutorial on Snowpipe Service - the automatic data ingestion mechanism in Snowflake. In this example, we will load JSON data from an AWS S3 bucket. However, Snowpipe works seamlessly with other data formats like CSV, Parquet, XML, and cloud storage providers like azure blob storage, and GCS (AWS & JSON is only a choice for the tutorial).</p><p>Setting up a Snowpipe is a 6 step process.</p><ol><li>Set up s3 bucket and store or pipe the data to flow into the bucket</li><li>Create the target Table to where the data should terminally go</li><li>Create an External Stage that points to the data source (S3 bucket)</li><li>Create the File Format for the data to be fetched</li><li>Create the Snowpipe</li><li>Configure S3 event notifications to notify Snowpipe when new data arrives</li></ol><p>That&rsquo;s all!</p><h2 id=tldr>tldr;<a class=anchor href=#tldr>#</a></h2><pre><code>-- Create target Table
CREATE TABLE ADDRESSDB.MASTER.USERDB (
    CREATETIME TIMESTAMP,
    FIRSTNAME STRING, 
    LASTNAME STRING, 
    PHONENO STRING, 
    EMAIL STRING, 
    ADDRESS VARIANT
    );

-- Create External Stage
CREATE STAGE ADDRESSDB.MASTER.USERDB_STAGE     URL='s3://bukent_name/prefix/'
CREDENTIALS = (AWS_KEY_ID = AWS_SECRET_KEY = '')
COPY_OPTIONS = (ON_ERROR='SKIP_FILE' PURGE = FALSE);

-- Create File Format
CREATE FILE FORMAT USERDB_FORMAT
  type = 'JSON'
  COMPRESSION = AUTO
  STRIP_OUTER_ARRAY = TRUE;

-- Create Snowpipe
CREATE PIPE ADDRESSDB.MASTER.USERDB_PIPE 
AUTO_INGEST = TRUE AS
COPY INTO ADDRESSDB.MASTER.USERDB 
FROM (SELECT CURRENT_TIMESTAMP::TIMESTAMP_NTZ, $1:&quot;firstname&quot;, $1:&quot;lastname&quot;, $1:&quot;phoneno&quot;, $1:&quot;email&quot;, $1:&quot;address&quot; FROM @ADDRESSDB.MASTER.USERDB_STAGE)
FILE_FORMAT = ADDRESSDB.MASTER.USERDB_FORMAT    
</code></pre><p>We will now go through them in a bit tad detail.</p><h2 id=step-1-setup-s3-bucket>Step 1: Setup S3 Bucket<a class=anchor href=#step-1-setup-s3-bucket>#</a></h2><p>Create an AWS S3 bucket. Take a note of the S3 bucket URL.
Now create an IAM User with permission to access this bucket and generate an Access Key and Access Secret.
Please follow the <a href=https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html>AWS S3 security best practices</a> while creating the bucket, IAM User, and access keys.</p><p>Take a note of the following details once you are on this step.</p><pre><code>Bucket URL: s3://&lt;bucket_name&gt;
Access Key: øøøøøøøøøøøøøøøø
Access Secret: øøøøøøøøøøøøøøøøø
</code></pre><h2 id=step-2-create-the-target-table>Step 2: Create the target Table<a class=anchor href=#step-2-create-the-target-table>#</a></h2><p>Login in to your Snowflake account, or to the SnowSQL prompt if you are a terminal person. Now create the target table to store the data.
In this tutorial, as an example, I am going to store a simple user database in Snowflake. A sample JSON document is as in the below snippet.</p><pre><code>[{
    &quot;firstname&quot;: &quot;Bruce&quot;, 
    &quot;lastname&quot;: &quot;Wayne&quot;, 
    &quot;phoneno&quot;: &quot;+12345678901&quot;, 
    &quot;email&quot;: &quot;bruce.wayne@wayneenterprises.com&quot;, 
    &quot;address&quot;: {&quot;street&quot;: &quot;Mountain Drive&quot;, &quot;doorno&quot;: 1007, &quot;city&quot;: &quot;Gotham&quot;}
}] 
</code></pre><p>The target table should essentially have these columns to effectively capture all of the data in our JSON document: FIRSTNAME, LASTNAME, PHONENO, EMAIL, and ADDRESS. The ADDRESS column will store a JSON object. Hence the data type of the field should be VARIANT. In addition to these, a CREATETIME column will record the time of insertion. The following SnowSQL command creates a table <code>USERDB</code> in the <code>ADDRESSDB</code> database under <code>MASTER</code> schema.</p><pre><code>CREATE TABLE ADDRESSDB.MASTER.USERDB (
    CREATETIME TIMESTAMP,
    FIRSTNAME STRING, 
    LASTNAME STRING, 
    PHONENO STRING, 
    EMAIL STRING, 
    ADDRESS VARIANT
    );
</code></pre><h2 id=step-3-create-the-external-stage>Step 3: Create the External Stage<a class=anchor href=#step-3-create-the-external-stage>#</a></h2><p>We will now configure an external stage in Snowflake to let it know about our S3 bucket, and the access credentials to copy data from it. Spin up your SnowSQL prompt again (or on one of your Snowflake Worksheet) and run the below command.</p><pre><code>CREATE STAGE ADDRESSDB.MASTER.USERDB_STAGE     URL='s3://bukent_name/prefix/'
CREDENTIALS = (AWS_KEY_ID = AWS_SECRET_KEY = '')
COPY_OPTIONS = (ON_ERROR='SKIP_FILE' PURGE = FALSE);
</code></pre><p>This command creates a stage called <code>USERDB_STAGE</code> that points to the S3 bucket <code>buket_name</code>. If you are storing your data with a specific S3 prefix you may add it to the URL (like s3://bucket_name/data/).
The credentials part configures the access credentials to read the data from S3 bucket. You may copy-paste the URL and access credentials if you have made a note of it in the first step. Please destroy the note after use and avoid unauthorized access.</p><p>The copy options specify additional configurations. The <code>on_error</code> option specifies the behavior on encountering an error in the data while loading. In the example, the copy options tell Snowflake to skip the file in case of errors. The purge option specifies whether the files are to be deleted from the external stage (the S3 bucket in this example) after loading. To see the complete list of <code>COPY_OPTIONS</code> please refer to the <a href=https://docs.snowflake.com/en/sql-reference/sql/create-stage.html>official documentation</a>.</p><h2 id=step-4-create-the-file-format>Step 4: Create the File Format<a class=anchor href=#step-4-create-the-file-format>#</a></h2><p>We need to create a file format to specify the data format expected at the External Stage. This is done using the <code>CREATE FILE FORMAT</code> command.</p><pre><code>CREATE FILE FORMAT USERDB_FORMAT
  type = 'JSON'
  COMPRESSION = AUTO
  STRIP_OUTER_ARRAY = TRUE;
</code></pre><p>Using this command we create a file format that can be used by a pipe later. The <code>type</code> parameter specifies that the expected format at the External Stage is JSON. The <code>compression</code> parameter is used to indicate the compression algorithm if the files on the External Stage are to be decompressed before reading. The strip-outer array parameter is used to strip the outer square brackets from the JSON document. This is a useful parameter and helps to load multiple JSON documents from a single file rather than loading file contents as a single JSON and later extracting the documents through SQL. Refer to the documentation for more details about File Formats.</p><h2 id=step-5-create-the-snowpipe>Step 5: Create the Snowpipe<a class=anchor href=#step-5-create-the-snowpipe>#</a></h2><p>In this step, we will create the Snowpipe specifying a file format for the data in the External Stage and defining the External Stage configurations and target to be used.</p><p>To create the Snowpipe, Snowflake provides the CREATE PIPE command.</p><pre><code>CREATE PIPE ADDRESSDB.MASTER.USERDB_PIPE 
AUTO_INGEST = TRUE AS
COPY INTO ADDRESSDB.MASTER.USERDB 
FROM (SELECT CURRENT_TIMESTAMP::TIMESTAMP_NTZ, $1:&quot;firstname&quot;, $1:&quot;lastname&quot;, $1:&quot;phoneno&quot;, $1:&quot;email&quot;, $1:&quot;address&quot; FROM @ADDRESSDB.MASTER.USERDB_STAGE)
FILE_FORMAT = ADDRESSDB.MASTER.USERDB_FORMAT
</code></pre><p>Using this command, we ask Snowflake to create a Snow pipe named USERDB_PIPE and automatically ingest data using this pipe whenever new data is available in the External Stage. The <code>SELECT</code> query in the configuration transforms or picks the specific values in the JSON document to load it into the target table. In the command above <code>CURRENT_TIMESTAMP::TIMESTAMP_NTZ</code> inserts the current timestamp in UTC timezone. Then, all fields in the JSON document are inserted using Snowflake&rsquo;s JSON querying syntax. Finally, we specify the file format as the <code>USERDB_FORMAT</code> we created in the previous step.</p><p>The pipe should be fully functional by now. You can test it by using the <code>refresh</code> query which will pick up any existing files in the S3 bucket and loads it into the <code>USERDB</code> table. The process may fail for JSON files if there are any errors or if there are any misconfigurations. Snowflake provides a few troubleshooting options in case of errors. I will write about it in the next post.</p><pre><code>ALTER PIPE ADDRESSDB.MASTER.USERDB_PIPE REFRESH
</code></pre><ol start=6><li>Configure S3 event notification</li></ol><hr><p>The final step is to create a mechanism to let Snowpipe know about the arrival of new data in the S3 bucket. Snowflake provides three options to do this. One is through a ReST endpoint run by the Snowpipe service that can receive notifications on new data arrival. This will require the Snowpipe service to always run the virtual warehouse to maintain the ReST endpoint. The next two options apply to AWS users. Similar solutions exist for GCP and Azure. The second method is to create an AWS SNS service to which Snowflake can subscribe for notifications. Another option is AWS SQS. We will look into this easier option in this example.</p><p>Once we create a Snow pipe, Snowflake creates an AWS SQS Queue for every configured Snowpipe. The ARN for the queue can be retrieved using the below command.</p><pre><code>show pipes;
</code></pre><p>This command will show the details of all Snowpipes created under the current database and schema. Copy the value under the <code>notification_channel</code> column for the <code>USERDB_PIPE</code> we just created.</p><p><img src=/img/pipe-arn.png alt="Show pipes result"></p><p>Now, login back to the S3 console and choose the source bucket&rsquo;s properties. Choose Events > Add Notification.
Under the events section, check the &lsquo;All Object creation events&rsquo; checkbox. Paste the <code>notification_channel</code> value in the <code>SQS queue ARN</code> field and Save.</p><p><img src=/img/sqs.png alt="S3 Event Configuration"></p><p>Phew, You are done!</p><p>Whenever a new JSON document gets added to the S3 bucket, Snowflake loads it into the USERDB table automagically. You can test this by uploading a valid JSON file to the bucket.</p><p>In case of load failures, please refer to the troubleshooting documentation here. A quick debugging hack is to issue the <code>copy into</code> SQL manually and check for any SQL related or permission errors. An implicit assumption made in this post is that the Snowflake user has a role attached with appropriate permissions to create the Table, External Stage, File Format and Snow pipe.</p></article><ul class=tags__list><li class=tag__item><a class=tag__link href=/tags/snowflake/>snowflake</a></li><li class=tag__item><a class=tag__link href=/tags/snowpipe/>snowpipe</a></li><li class=tag__item><a class=tag__link href=/tags/tutorial/>tutorial</a></li><li class=tag__item><a class=tag__link href=/tags/tech/>tech</a></li></ul><div class=pagination><a class=pagination__item href=/blog/hello-world/><span class=pagination__label>Previous Post</span>
<span class=pagination__title>Hello World</span></a>
<a class=pagination__item href=/blog/snowflake-json-capabilities/><span class=pagination__label>Next Post</span>
<span class=pagination__title>Snowflake JSON Capabilities</a></a></div><script src=https://utteranc.es/client.js repo=vishnuvp/blog issue-term=title label=Comment theme=github-light crossorigin=anonymous async></script><footer class=post__footer><div class=social-icons><a class=social-icons__icon title=Twitter style=background-image:url(/images/social/twitter.svg) href=https://twitter.com/vishnuvp target=_blank rel=noopener></a><a class=social-icons__icon title=GitHub style=background-image:url(/images/social/github.svg) href=https://github.com/vishnuvp target=_blank rel=noopener></a><a class=social-icons__icon title=Email style=background-image:url(/images/social/email.svg) href=mailto:devinbox.vishnu@gmail.com target=_blank rel=noopener></a><a class=social-icons__icon title=Instagram style=background-image:url(/images/social/instagram.svg) href=https://instagram.com/vishnuvenukumar target=_blank rel=noopener></a><a class=social-icons__icon title=LinkedIn style=background-image:url(/images/social/linkedin.svg) href=https://www.linkedin.com/in/vishnuvenukumar target=_blank rel=noopener></a></div><p>© vishnuvp 2020</p></footer></div></div><div class=toc-container><nav id=TableOfContents><ul><li><a href=#tldr>tldr;</a></li><li><a href=#step-1-setup-s3-bucket>Step 1: Setup S3 Bucket</a></li><li><a href=#step-2-create-the-target-table>Step 2: Create the target Table</a></li><li><a href=#step-3-create-the-external-stage>Step 3: Create the External Stage</a></li><li><a href=#step-4-create-the-file-format>Step 4: Create the File Format</a></li><li><a href=#step-5-create-the-snowpipe>Step 5: Create the Snowpipe</a></li></ul></nav></div></div></main><script src=/js/index.min.49e4d8a384357d9b445b87371863419937ede9fa77737522ffb633073aebfa44.js integrity="sha256-SeTYo4Q1fZtEW4c3GGNBmTft6fp3c3Ui/7YzBzrr+kQ=" crossorigin=anonymous></script><script src=https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js></script><script src=https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js data-autoloader-path=https://unpkg.com/prismjs@1.20.0/components/></script><script src=/js/table-of-contents.js></script></body></html>